---
title: "WSL2ì—ì„œ Spark ì„¤ì¹˜í•˜ê¸°"
author: "Hanjeongin"
date: 2022-04-18 18:00:00
tags: Settings
---

## í•„ìˆ˜ íŒŒì¼ ì„¤ì¹˜

- ìë°” ë° spark íŒŒì¼ ì„¤ì¹˜
- ì´ë¯¸ í–ˆì„ ê²½ìš° ê·¸ëƒ¥ ë„˜ê²¨ë„ ë¨

```jsx
sudo apt-get install openjdk-8-jdk
sudo wget https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
sudo tar -xvzf spark-3.2.0-bin-hadoop3.2.tgz
```

- vi ~/.bashrc ì…ë ¥ í›„ ë‹¤ìŒ ì½”ë“œ ì…ë ¥

```jsx
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export SPARK_HOME=/mnt/c/hadoop/spark-3.2.0-bin-hadoop3.2
export PATH=$JAVA_HOME/bin:$PATH
export PATH=$SPARK_HOME/bin:$PATH
export PYSPARK_PYTHON=/usr/bin/python3
```

## ****.bashrc íŒŒì¼ ìˆ˜ì •****

- source ~/.bashrc ì…ë ¥í•´ì„œ ì ìš©
    
    ![Untitled](/images/Spark_install_in_WSL2/Untitled.png)
    
- ê²½ë¡œ(`/mnt/c/hadoop/spark-3.2.0-bin-hadoop3.2`)ë¥¼ ì¡ì•„ì£¼ê³  pyspark ì‹¤í–‰
    
    ![Untitled](/images/Spark_install_in_WSL2/Untitled%201.png)
    
- ë‹¤ìŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì„œ ì˜ ì„¤ì¹˜ëëŠ”ì§€ í…ŒìŠ¤íŠ¸

```jsx
rd = sc.textFile("README.md")
rd.count()
```

![Untitled](/images/Spark_install_in_WSL2/Untitled%202.png)

## UIë¡œ í™•ì¸í•˜ê¸°

- ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„± (ì•„ë¬´ê³³ì— í•´ë„ ìƒê´€ ì—†ìŒ)
- ê°€ìƒí™˜ê²½ ì„¤ì¹˜
    
    ![Untitled](/images/Spark_install_in_WSL2/Untitled%203.png)
    
- pyspark ì„¤ì¹˜
    
    ![Untitled](/images/Spark_install_in_WSL2/Untitled%204.png)
    
- ë””ë ‰í† ë¦¬ ë°‘ì— data ë””ë ‰í† ë¦¬ ë§Œë“¤ê³  README.mdì— ë‹¤ìŒ ë‚´ìš© ì…ë ¥

<aside>
ğŸ’¡ This program just counts the number of lines containing â€˜aâ€™ and the number containing â€˜bâ€™ in a text file. Note that youâ€™ll need to replace YOUR_SPARK_HOME with the location where Spark is installed. As with the Scala and Java examples, we use a SparkSession to create Datasets. For applications that use custom classes or third-party libraries, we can also add code dependencies to spark-submit through its --py-files argument by packaging them into a .zip file (see spark-submit --help for details). SimpleApp is simple enough that we do not need to specify any code dependencies.

</aside>

![Untitled](/images/Spark_install_in_WSL2/Untitled%205.png)

- ë‹¤ì‹œ ìƒìœ„ ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•´ì„œ SimpleApp.pyì— ë‹¤ìŒ ì½”ë“œ ì…ë ¥

```jsx
from pyspark.sql import SparkSession

logFile = "data/README.md"  # Should be some file on your system
spark = SparkSession.builder.appName("SimpleApp").getOrCreate()
logData = spark.read.text(logFile).cache()

numAs = logData.filter(logData.value.contains('a')).count()
numBs = logData.filter(logData.value.contains('b')).count()

print("Lines with a: %i, lines with b: %i" % (numAs, numBs))

input("Typing....")

spark.stop()
```

- `$SPARK_HOME/bin/spark-submit --master local[4] [SimpleApp.py](http://simpleapp.py/)` ì…ë ¥
    
    ![Untitled](/images/Spark_install_in_WSL2/Untitled%206.png)
    
- ì²« ì¤„ì— ë³´ë©´ using ë‹¤ìŒ ì£¼ì†Œê°€ ìˆìŒ
- ì£¼ì†Œë¥¼ ë³µì‚¬í•œ ë’¤ ì¸í„°ë„·ì°½ì—ì„œ ë¶™ì—¬ë„£ê³  ë’¤ì— :4040ì„ ë¶™ì„ (ì—¬ê¸°ì„  [172.31.145.15:4040](http://172.31.145.15:4040/jobs/))
- UIë¡œ í™•ì¸í•  ìˆ˜ ìˆìŒ

![Untitled](/images/Spark_install_in_WSL2/Untitled%207.png)