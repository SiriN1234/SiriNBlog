---
title: "마켓 머신러닝"
author: "Hanjeongin"
date: 2022-03-27 22:53:00
tags: Python
---

출처 : 혼자 공부하는 머신러닝+딥러닝

# 마켓과 머신러닝

- 한빛 마켓을 좋은 물건으로 인기가 높은 앱 마켓
- 직원들이 생선 이름을 외우지 못하는 사태가 발생
- 생선 이름을 자동으로 알려주는 머신러닝을 만들어야함

## 생선 분류 문제

- 생선 종류 : 도미, 곤들매기, 농어, 강꼬치고기, 로치, 빙어, 송어
- 생선을 구분하기 위해서 특징을 알아야 함
- 생선 길이가 30cm 이상이면 도미


```python
# 도미를 구분하는 코드

#if (fish_length >= 30) :
#  print("도미")
```

- 30cm보다 크다고 무조건 도미라고 말할 수 없음
- 도미의 길이가 전부 같을 수도 없음
- 보통 프로그램은 '누군가 정해준 기준대로 일'을 함
- 머신러닝은 누구도 알려주지 않는 기준을 찾아서 일을 함
- 다시 말해 누가 말해주지 않아도 "30~40cm 길이의 생선은 도미이다"라는 기준을 찾음

## 도미 데이터 준비하기

- 물류창고에선 무게와 길이를 함께 재어줌
- 도미와 빙어를 먼저 비교


```python
# 35마리의 도미 데이터

# 생선의 길이
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]

# 생선의 무게
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]
```

- 각 도미의 특징을 길이와 무게로 표현한 것
- 이런 특징을 특성(feature)이라 부름

- 두 특성을 숫자로 보는 것 보다 그래프로 표현하면 데이터를 잘 이해할 수 있고, 앞으로 할 작업에 대해 힌트를 얻을 수 있음
- 길이를 x축으로, 무게를 y축으로 정함
- 각 도미를 그래프에 점으로 표시함
- 이런 그래프를 산점도(scatter plot)라고 부름
- 파이썬에서 그래프를 그리는 대표적인 패키지는 matplotlib
- 산점도를 그리는 함수는 scatter()
- import는 패키지를 사용하기 위해 불러오는 명령


```python
# 도미의 특성을 산점도로 그리기

import matplotlib.pyplot as plt # matplotlib의 pyplot 함수를 plt로 줄여서 사용

plt.scatter(bream_length, bream_weight)
plt.xlabel('length') # x축은 길이
plt.ylabel('weight') # y축은 무게
plt.show()
```


    
![png](/images/market_machine_learning/output_8_0.png)
    


- 2개의 특성을 사용해 그린 그래프이기에 2차원 그래프
- 산점도 그래프가 일직선에 가까운 형태로 나타나는 경우를 선형(linear)적이라고 말함

## 빙어 데이터 준비하기

- 빙어는 14마리


```python
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]

smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

- 도미와 빙어 데이터를 함께 산점도로 그리기
- scatter() 함수를 연달아 사용하면 됨


```python
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')

plt.show()
```


    
![png](/images/market_machine_learning/output_13_0.png)
    


- 주황색 점이 빙어의 산점도
- 빙어는 길이가 늘더라도 무게가 많이 늘지는 않음
- 따라서 선형적이지만, 무게가 길이에 영향을 덜 받는다고 볼 수 있음

## 첫 번째 머신러닝 프로그램

- k-최근접 이웃(k-Nearest Neighbors) 알고리즘을 사용해 도미와 빙어 데이터를 구분
- 사용하기 전에 도미와 빙어 데이터를 하나의 데이터로 합쳐줌


```python
length = bream_length + smelt_length
weight = bream_weight + smelt_weight
```

- 사용할 머신러닝 패키지는 사이킷런(scikit-learn)
- 사용하려면 각 특성의 리스트를 세로방향으로 늘어뜨린 2차원 리스트를 만들어야 함
- zip() 함수와 리스트 내포(list somprehension)구문을 사용하면 만들 수 있음
- zip() 함수는 나열된 리스트 각각에서 하나씩 원소를 꺼내 반환


```python
fish_data = [[l, w] for l, w in zip(length, weight)]
```

- zip() 함수로 length와 wieght 리스트에서 원소를 하나씩 꺼내어 l과 w에 할당
- 그러면 [l, w]가 하나의 원소로 구성된 리스트가 만들어 짐


```python
print(fish_data)
```

    [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]]
    

- 이런 리스트를 2차원 리스트 혹은 리스트의 리스트라고 부름
- 각각 어떤 생선인지 답을 만드는 정답 데이터가 필요함
- 도미와 빙어를 숫자 1과 0으로 표현
- 1부터 35까지는 도미이므로 1, 36부터 49까지는 빙어이므로 0


```python
fish_target = [1] * 35 + [0] * 14
print(fish_target)
```

    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    

- 사이킷런 패키지에서 k-최근접 이웃 알고리즘을 구현한 클래스인 KNeighborsClassifier를 임포트


```python
from sklearn.neighbors import KNeighborsClassifier
```

- 파이썬에서 패키지나 모듈 전체를 임포트하지 않고 특정 클래스만 임포트 하려면 from ~ import 구문을 사용함
- 이렇게 하면 다음과 같이 클래스 이름을 길게 사용하지 않아도 됨


```python
# import sklearn
# model = sklear.neighbors.KNeighborsClassifier()
```

- 임포트한 KNeighborsClassifier 클래스의 객체를 만듬


```python
kn = KNeighborsClassifier()
```

- 이 객체에 fish_data와 fish_target을 전달하여 도미를 찾기 위한 기준을 학습시킴
- 이런 과정을 머신러닝에서는 훈련(training이라고 부름)
- 사이킷런에서는 fit()메서드가 이런 역할을 함


```python
# fish_data와 fish_target을 순서대로 전달

kn.fit(fish_data, fish_target)
```




    KNeighborsClassifier()



- fit() 메서드는 주어진 데이터로 알고리즘을 훈련함
- 이제 객체 kn이 얼마나 잘 훈련 되었는지 평가를 해 봄
- 사이킷런에서 모델을 평가하는 메서드는 score()
- score()는 0에서 1 사이의 값을 반환
- 1은 모든 데이터를 정확히 맞췄다는 것을 나타냄


```python
kn.score(fish_data, fish_target)
```




    1.0



- 이 값을 정확도라고 부름
- 이 모델은 정확도 100%

## k-최근접 이웃 알고리즘

- 이 알고리즘은 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용


```python
kn.predict([[30, 600]])
```




    array([1])



- predict() 메서드는 새로운 데이터의 정답을 예측
- 이 메서드도 fit()과 마찬가지로 2차원 리스트를 전달해야 함
- 그래서 값을 리스트 2번으로 감쌈
- 결과값이 1이기 때문에 입력된 생선 데이터는 도미

- k-최근접 이웃 알고리즘을 위해 준비해야 할 일은 데이터를 모두 가지고 있는 것이 전부
- 새로운 데이터에 대해 예측할 때는 가장 가까운 직선거리에 어떤 데이터가 있는지를 살피기만 하면 됨
- 단점은 데이터가 아주 많은 경우에는 사용하기 어려움
- 데이터가 크기 때문에 메모리가 많이 필요하고, 직선거리를 계산하는데도 많은 시간이 필요함

- 사이킷런의 KNeighborsClassfier 클래스는 _fit_X 속성에 전달한 fish_data를 모두 가지고 있고, _y 속성에 fish_target을 가지고 있음


```python
print(kn._fit_X)
```

    [[  25.4  242. ]
     [  26.3  290. ]
     [  26.5  340. ]
     [  29.   363. ]
     [  29.   430. ]
     [  29.7  450. ]
     [  29.7  500. ]
     [  30.   390. ]
     [  30.   450. ]
     [  30.7  500. ]
     [  31.   475. ]
     [  31.   500. ]
     [  31.5  500. ]
     [  32.   340. ]
     [  32.   600. ]
     [  32.   600. ]
     [  33.   700. ]
     [  33.   700. ]
     [  33.5  610. ]
     [  33.5  650. ]
     [  34.   575. ]
     [  34.   685. ]
     [  34.5  620. ]
     [  35.   680. ]
     [  35.   700. ]
     [  35.   725. ]
     [  35.   720. ]
     [  36.   714. ]
     [  36.   850. ]
     [  37.  1000. ]
     [  38.5  920. ]
     [  38.5  955. ]
     [  39.5  925. ]
     [  41.   975. ]
     [  41.   950. ]
     [   9.8    6.7]
     [  10.5    7.5]
     [  10.6    7. ]
     [  11.     9.7]
     [  11.2    9.8]
     [  11.3    8.7]
     [  11.8   10. ]
     [  11.8    9.9]
     [  12.     9.8]
     [  12.2   12.2]
     [  12.4   13.4]
     [  13.    12.2]
     [  14.3   19.7]
     [  15.    19.9]]
    


```python
print(kn._y)
```

    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
     0 0 0 0 0 0 0 0 0 0 0 0]
    

- k-최근접 이웃 알고리즘은 새로운 데이터가 등장하면 가장 가까운 데이터를 참고하여 구분을 함
- KNeighborsClassifier 클래스의 기본값은 5
- n_neighbors 매개변수로 변경 가능


```python
# 참고 데이터를 49개로 한 kn49 모델

kn49 = KNeighborsClassifier(n_neighbors = 49)
```


```python
kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)
```




    0.7142857142857143




```python
# kn49 모델은 도미만 올바르게 맞히기 때문에 다음과 같이 정확도를 계산하면 score() 메서드와 같은 값을 얻을 수 있음

print(35/49)
```

    0.7142857142857143
    

## 도미와 빙어 분류 (문제해결 과정)

- 도미와 빙어를 구분하기 위해 머신러닝 프로그램을 만드는 과정
- 먼저 도미와 빙어의 길이와 무게를 측정해서 파이썬 리스트로 만듦
- 사용한 알고리즘은 k-최근접 이웃 알고리즘
- 사이킷런의 k-최근접 이웃 알고리즘은 주변에서 가장 가까운 5개의 데이터를 보고 다수결의 원칙에 따라 데이터를 예측
- 도미와 빙어를 분류하는 문제를 풀면서 KNeighborsClassifier 클래스의 fit(), score(), predict() 메서드를 사용해 보았음
- 끝으로 k-최근접 이웃 알고리즘을 특징을 알아봤음

## 전체 소스 코드

- 손코딩을 모아놓은 코드


```python
# 손코딩 부분에 없었던 부분만 표시하고 나머지는 주석처리 했음

"""
# 마켓과 머신러닝
"""

"""
## 생선 분류 문제
"""

"""
### 도미 데이터 준비하기
"""

"""
# 35마리의 도미 데이터

# 생선의 길이
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]

# 생선의 무게
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]

# 도미의 특성을 산점도로 그리기

import matplotlib.pyplot as plt # matplotlib의 pyplot 함수를 plt로 줄여서 사용

plt.scatter(bream_length, bream_weight)
plt.xlabel('length') # x축은 길이
plt.ylabel('weight') # y축은 무게
plt.show()

"""

"""
### 빙어 데이터 준비하기
"""

"""
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]

smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]

plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')

plt.show()

"""

"""
## 첫 번째 머신러닝 프로그램
"""

"""
length = bream_length + smelt_length
weight = bream_weight + smelt_weight

fish_data = [[l, w] for l, w in zip(length, weight)]

print(fish_data)

fish_target = [1] * 35 + [0] * 14
print(fish_target)

from sklearn.neighbors import KNeighborsClassifier

# fish_data와 fish_target을 순서대로 전달

kn.fit(fish_data, fish_target)

kn.score(fish_data, fish_target)

"""

"""
### k-최근접 이웃 알고리즘
"""

"""
kn.predict([[30, 600]])

print(kn._fit_X)

print(kn._y)

# 참고 데이터를 49개로 한 kn49 모델

kn49 = KNeighborsClassifier(n_neighbors = 49)

kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)

# kn49 모델은 도미만 올바르게 맞히기 때문에 다음과 같이 정확도를 계산하면 score() 메서드와 같은 값을 얻을 수 있음

print(35/49)
"""

# 그래프만 그려져 있고, 손 코딩 부분에 없던 부분

plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.scatter(30, 600, marker = '^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](/images/market_machine_learning/output_47_0.png)
    


# 마무리

## 키워드로 끝내는 핵심 포인트

- **특성**은 데이터를 표현하는 하나의 성질. 이 절에서 생선 데이터 각각을 길이와 무게 특성으로 나타냈음.
- 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정을 **훈련**이라고 함. 사이킷런에서는 fit() 메서드가 하는 역할.
- **k-최근접 이웃 알고리즘**은 가장 간단한 머신러닝 알고리즘 중 하나. 사실 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부.
- 머신러닝 프로그램에서는 알고리즘이 구현된 객체를 **모델**이라고 부름. 종종 알고리즘 자체를 모델이라고 부르기도 함.
- **정확도**는 정확한 답을 볓 개 맞혔는지를 백분율로 나타낸 값. 사이킷런에서는 0~1 사이의 값으로 출력 됨.
- 정확도 = (정확히 맞힌 개수) / (전체 데이터 개수)

## 핵심 패키지와 함수

### matplotlib

- **scatter()**는 산점도를 그리는 맷플롯 라이브러리 함수. 처음 2개의 매개변수로 x축 값과 y축 값을 전달함. 이 값은 파이썬 리스트 또는 넘파이 배열.
- c 매개변수로 색깔을 지정함. RGB를 16진수로 지정하거나 색깔 코드 'b'(파랑), 'g'(초록), 'r'(빨강), 'c'(시안), 'm'(마젠타), 'y'(노랑), 'k'(검정), 'w'(흰색) 중 하나를 지정함 지정하지 않을 경우 10개의 기본 색깔을 사용해 그래프를 그림
- marker 매개변수로 마커 스타일을 지정. marker의 기본값은 o(circle, 원).

### sckit-learn

- **KNeighborsClassifier()는 k-최근접 이웃 분류 무델을 만드는 사이킷런 클래스. n_neighbors 매개변수로 이웃의 개수를 지정함. 기본값은 5
- p 매개변수로 거리를 재는 방법을 지정함. 1일 경우 맨해튼거리를 사용하고 2일 경우 유클리디안 거리를 사용함. 기본값은 2
- n_jobs 매개변수로 사용할 cpu 코어를 지정할 수 있음. -1로 설정하면 모든 CPU 코어를 사용함. 이웃 간의 거리 계산 속도를 눞일 수 있지만 fir() 메서드에는 영향이 없음. 기본값은 1
- **fit()**은 사이킷런 모델을 훈련할 때 사용하넌 메서드. 처음 두 매개변수로 훈련에 사용할 특성과 정답 데이터를 전달함.
- **predict()**는 사이킷런 모델을 훈련하고 예측할 떄 사용하는 메서드. 특성 데이터 하나만 매개변수로 받음
- **score()**는 훈련된 사이킷런 모델의 성능을 측정함. 처음 두 매개변수로 특성과 정답 데이터를 전달함. 이 메서드는 먼저 predict() 메서드로 예측을 수행한 다음 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율을 변환.

# 확인문제

## 1. 데이터를 표현하는 하나의 성질로써, 예를 들어 국가 데이터의 경우 인구 수, GDP, 면적 등이 하나의 국가를 나타냅니다. 머신러닝에서 이런 성질을 무엇이라고 부르나요?
- 1. 특성
- 2. 특질
- 3. 개성
- 4. 요소

### 답 : 1번

## 2. 가장 가까운 이웃을 참고하여 정답을 예측하는 알고리즘의 구현된 사이킷런 클래스는 무엇인가요?
- 1. SGDClassifier
- 2. LinearRegression
- 3. RandomForestClassifier
- 4. KNeighborsClassifier

## 답 : 4번

## 3. 사이킷런 모델을 훈련할 때 사용하는 메서드는 어떤 것인가요?

- 1. predict()
- 2. fit()
- 3. score()
- 4. transform()

### 답 : 2번

## 4. 본문(56쪽)에서 n_neighbors를 49로 설정했을 때 점수가 1.0보다 작았습니다. 즉, 정확도가 100%가 아닙니다. 그럼 n_neighbors의 기본값인 5부터 49까지 바꾸어 가며 점수가 1.0 아래로 내려가기 시작하는 이웃의 개수를 찾아보세요. 이 문제를 위해 KNeighborsClassifier 클래스 객체를 매번 다시 만들 필요는 없습니다. 심지어 fit() 메서드로 훈련을 다시 할 필요도 없습니다. k-최근접 이웃 알고리즘의 훈련은 데이터를 저장하는 것이 전부이기 때문입니다. KNdighborsClasfier 클래스의 이웃 개수는 모델 객체의 n_neighbors 속성으로 바꿀 수 있습니다. 이웃 개수를 바꾼 후 score(0 메서드로 다시 계산하시기만 하면 됩니다.


```python
kn = KNeighborsClassifier()
kn.fit(fish_data, fish_target)

for n in range(5, 50) :
  # k-최근접 이웃 개수 설정
  kn.n_neighbors = 18

  # 점수 계산
  score = kn.score(fish_data, fish_target)

  # 100% 정확도에 미치지 못하는 이웃 개수 출력
  if (score < 1) :
    print(n, score)
    break
```

    5 0.9795918367346939
    

### 답 : 17
